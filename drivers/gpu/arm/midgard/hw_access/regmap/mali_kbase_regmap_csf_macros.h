/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 *
 * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
 *
 * This program is free software and is provided to you under the terms of the
 * GNU General Public License version 2 as published by the Free Software
 * Foundation, and any use by you of this program is subject to the terms
 * of such GNU license.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 */

#ifndef _MALI_KBASE_REGMAP_CSF_MACROS_H_
#define _MALI_KBASE_REGMAP_CSF_MACROS_H_

#if !MALI_USE_CSF
#error "Cannot be compiled with JM"
#endif

#define ENUM_OFFSET(_index, _base, _next) (_base + _index * (_next - _base))


#define GPU_CONTROL_ENUM(regname) GPU_CONTROL__##regname
#define GPU_TEXTURE_FEATURES_ENUM(n) GPU_CONTROL_ENUM(TEXTURE_FEATURES_##n)
#define GPU_TEXTURE_FEATURES_OFFSET(n) (GPU_TEXTURE_FEATURES_ENUM(0) + n)
#define GPU_ASN_HASH_ENUM(n) GPU_CONTROL_ENUM(ASN_HASH_##n)
#define GPU_ASN_HASH_OFFSET(n) (GPU_ASN_HASH_ENUM(0) + n)
#define GPU_SYSC_PBHA_OVERRIDE_ENUM(n) GPU_CONTROL_ENUM(SYSC_PBHA_OVERRIDE##n)
#define GPU_SYSC_PBHA_OVERRIDE_OFFSET(n) (GPU_SYSC_PBHA_OVERRIDE_ENUM(0) + n)
#define GPU_SYSC_ALLOC_ENUM(n) GPU_CONTROL_ENUM(SYSC_ALLOC##n)
#define GPU_SYSC_ALLOC_OFFSET(n) (GPU_SYSC_ALLOC_ENUM(0) + n)

/* GPU_L2_SLICE_HASH_OFFSET aliasing GPU_ASN_HASH_OFFSET */
#define GPU_L2_SLICE_HASH_OFFSET(n) GPU_ASN_HASH_OFFSET(n)

#define JOB_CONTROL_ENUM(regname) JOB_CONTROL__##regname

#define MMU_CONTROL_ENUM(regname) MMU_CONTROL__##regname
#define MMU_AS_ENUM(n, regname) MMU_CONTROL_ENUM(AS##n##__##regname)
#define MMU_AS_BASE_ENUM(n) MMU_AS_ENUM(n, TRANSTAB)
#define MMU_AS_OFFSET(n, regname) ENUM_OFFSET(n, MMU_AS_ENUM(0, regname), MMU_AS_ENUM(1, regname))
#define MMU_AS_BASE_OFFSET(n) MMU_AS_OFFSET(n, TRANSTAB)

#define USER_ENUM(regname) USER__##regname

#define IPA_CONTROL_ENUM(regname) IPA_CONTROL__##regname
#define IPA_VALUE_CSHW_ENUM(n) IPA_CONTROL_ENUM(VALUE_CSHW_##n)
#define IPA_VALUE_CSHW_OFFSET(n) (IPA_VALUE_CSHW_ENUM(0) + n)
#define IPA_VALUE_MEMSYS_ENUM(n) IPA_CONTROL_ENUM(VALUE_MEMSYS_##n)
#define IPA_VALUE_MEMSYS_OFFSET(n) (IPA_VALUE_MEMSYS_ENUM(0) + n)
#define IPA_VALUE_TILER_ENUM(n) IPA_CONTROL_ENUM(VALUE_TILER_##n)
#define IPA_VALUE_TILER_OFFSET(n) (IPA_VALUE_TILER_ENUM(0) + n)
#define IPA_VALUE_SHADER_ENUM(n) IPA_CONTROL_ENUM(VALUE_SHADER_##n)
#define IPA_VALUE_SHADER_OFFSET(n) (IPA_VALUE_SHADER_ENUM(0) + n)

#define GPU_GOVERNOR_ENUM(regname) GPU_GOVERNOR__##regname
#define GOV_IPA_CONTROL_ENUM(regname) GPU_GOVERNOR_ENUM(GOV_IPA_CONTROL__##regname)
#define IPA_VALUE_NEURAL_ENUM(n) GOV_IPA_CONTROL_ENUM(VALUE_NEURAL_##n)
#define IPA_VALUE_NEURAL_OFFSET(n) (IPA_VALUE_NEURAL_ENUM(0) + n)

#define HOST_POWER_ENUM(regname) GPU_CONTROL_ENUM(HOST_POWER__##regname)

#define DOORBELL_BLOCK_ENUM(n, regname) DOORBELL_BLOCK_##n##__##regname
#define DOORBELL_BLOCK_OFFSET(n, regname) \
	ENUM_OFFSET(n, DOORBELL_BLOCK_ENUM(0, regname), DOORBELL_BLOCK_ENUM(1, regname))

/* register value macros */

/* L2_CONFIG PBHA values */
#define L2_CONFIG_PBHA_HWU_SHIFT GPU_U(12)
#define L2_CONFIG_PBHA_HWU_MASK (GPU_U(0xF) << L2_CONFIG_PBHA_HWU_SHIFT)
#define L2_CONFIG_PBHA_HWU_GET(reg_val) \
	(((reg_val)&L2_CONFIG_PBHA_HWU_MASK) >> L2_CONFIG_PBHA_HWU_SHIFT)
#define L2_CONFIG_PBHA_HWU_SET(reg_val, value)    \
	(((reg_val) & ~L2_CONFIG_PBHA_HWU_MASK) | \
	 (((value) << L2_CONFIG_PBHA_HWU_SHIFT) & L2_CONFIG_PBHA_HWU_MASK))

#define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT (0)
#define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_MASK \
	((0xFFU) << PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT)
#define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_GET(reg_val)         \
	(((reg_val)&PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_MASK) >> \
	 PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT)

/*
 * Begin AARCH64 MMU TRANSTAB register values
 */
#define AS_TRANSTAB_BASE_SHIFT GPU_U(0)
#define AS_TRANSTAB_BASE_MASK (GPU_ULL(0xFFFFFFFFFFFFFFFF) << AS_TRANSTAB_BASE_SHIFT)
#define AS_TRANSTAB_BASE_GET(reg_val) (((reg_val)&AS_TRANSTAB_BASE_MASK) >> AS_TRANSTAB_BASE_SHIFT)
#define AS_TRANSTAB_BASE_SET(reg_val, value)     \
	(~(~(reg_val) | AS_TRANSTAB_BASE_MASK) | \
	 (((uint64_t)(value) << AS_TRANSTAB_BASE_SHIFT) & AS_TRANSTAB_BASE_MASK))

/*
 * Begin MMU STATUS register values
 */
#define AS_STATUS_AS_ACTIVE_INT_SHIFT GPU_U(1)
#define AS_STATUS_AS_ACTIVE_INT_MASK (GPU_U(0x1) << AS_STATUS_AS_ACTIVE_INT_SHIFT)
#define AS_STATUS_AS_ACTIVE_INT_GET(reg_val) \
	(((reg_val)&AS_STATUS_AS_ACTIVE_INT_MASK) >> AS_STATUS_AS_ACTIVE_INT_SHIFT)
#define AS_STATUS_AS_ACTIVE_INT_SET(reg_val, value)     \
	(~(~(reg_val) | AS_STATUS_AS_ACTIVE_INT_MASK) | \
	 (((value) << AS_STATUS_AS_ACTIVE_INT_SHIFT) & AS_STATUS_AS_ACTIVE_INT_MASK))

/*
 * Begin MMU FAULTSTATUS register values
 */
#define AS_FAULTSTATUS_EXCEPTION_TYPE_OK 0x0
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TERMINATED 0x4
#define AS_FAULTSTATUS_EXCEPTION_TYPE_KABOOM 0x5
#define AS_FAULTSTATUS_EXCEPTION_TYPE_EUREKA 0x6
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ACTIVE 0x8
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_RESOURCE_TERMINATED 0xF
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_CONFIG_FAULT 0x40
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_UNRECOVERABLE 0x41
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_ENDPOINT_FAULT 0x44
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_BUS_FAULT 0x48
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_INVALID_INSTRUCTION 0x49
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_CALL_STACK_OVERFLOW 0x4A
#define AS_FAULTSTATUS_EXCEPTION_TYPE_CS_INHERIT_FAULT 0x4B
#define AS_FAULTSTATUS_EXCEPTION_TYPE_INSTR_INVALID_PC 0x50
#define AS_FAULTSTATUS_EXCEPTION_TYPE_INSTR_INVALID_ENC 0x51
#define AS_FAULTSTATUS_EXCEPTION_TYPE_INSTR_BARRIER_FAULT 0x55
#define AS_FAULTSTATUS_EXCEPTION_TYPE_RT_STACK_OVERFLOW 0x56
#define AS_FAULTSTATUS_EXCEPTION_TYPE_DATA_INVALID_FAULT 0x58
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TILE_RANGE_FAULT 0x59
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDR_RANGE_FAULT 0x5A
#define AS_FAULTSTATUS_EXCEPTION_TYPE_IMPRECISE_FAULT 0x5B
#define AS_FAULTSTATUS_EXCEPTION_TYPE_OUT_OF_MEMORY 0x60
#define AS_FAULTSTATUS_EXCEPTION_TYPE_NE_DATA_INVALID_FAULT 0x61
#define AS_FAULTSTATUS_EXCEPTION_TYPE_NE_ADDR_RANGE_FAULT 0x62
#define AS_FAULTSTATUS_EXCEPTION_TYPE_NE_TSU_SPACE_FAULT 0x63
#define AS_FAULTSTATUS_EXCEPTION_TYPE_NE_TSU_INVALID_ENC 0x64
#define AS_FAULTSTATUS_EXCEPTION_TYPE_NE_WEIGHT_STREAM_ERROR 0x65
#define AS_FAULTSTATUS_EXCEPTION_TYPE_FIRMWARE_INTERNAL_ERROR 0x68
#define AS_FAULTSTATUS_EXCEPTION_TYPE_RESOURCE_EVICTION_TIMEOUT 0x69
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_0 0x70
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_1 0x71
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_2 0x72
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_3 0x73
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_4 0x74
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_5 0x75
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_6 0x76
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_7 0x77
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_8 0x78
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_9 0x79
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_10 0x7A
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_11 0x7B
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_12 0x7C
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_13 0x7D
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_14 0x7E
#define AS_FAULTSTATUS_EXCEPTION_TYPE_SW_FAULT_15 0x7F
#define AS_FAULTSTATUS_EXCEPTION_TYPE_GPU_BUS_FAULT 0x80
#define AS_FAULTSTATUS_EXCEPTION_TYPE_GPU_SHAREABILITY_FAULT 0x88
#define AS_FAULTSTATUS_EXCEPTION_TYPE_GPU_CACHEABILITY_FAULT 0x8A
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_0 0xC0
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_1 0xC1
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_2 0xC2
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_3 0xC3
#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_4 0xC4
#define AS_FAULTSTATUS_EXCEPTION_TYPE_PERMISSION_FAULT_0 0xC8
#define AS_FAULTSTATUS_EXCEPTION_TYPE_PERMISSION_FAULT_1 0xC9
#define AS_FAULTSTATUS_EXCEPTION_TYPE_PERMISSION_FAULT_2 0xCA
#define AS_FAULTSTATUS_EXCEPTION_TYPE_PERMISSION_FAULT_3 0xCB
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ACCESS_FLAG_1 0xD9
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ACCESS_FLAG_2 0xDA
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ACCESS_FLAG_3 0xDB
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_IN 0xE0
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT0 0xE4
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT1 0xE5
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT2 0xE6
#define AS_FAULTSTATUS_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT3 0xE7
#define AS_FAULTSTATUS_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_0 0xE8
#define AS_FAULTSTATUS_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_1 0xE9
#define AS_FAULTSTATUS_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_2 0xEA
#define AS_FAULTSTATUS_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_3 0xEB

/*
 * Begin MMU MEMATTR register values
 */
#define AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SHARED 0x0

/* NEURAL_CONTROL register */
#define NEURAL_CONTROL_LO_LATENCY_LIMIT_SHIFT 0
#define NEURAL_CONTROL_LO_LATENCY_LIMIT_MASK (0x7ul)

#define NEURAL_CONTROL_HI_LATENCY_LIMIT_SHIFT 3
#define NEURAL_CONTROL_HI_LATENCY_LIMIT_MASK (0x38ul)

#define NEURAL_CONTROL_LATENCY_LIMIT_MAX_VALUE 7

#define NEURAL_CONTROL_MAC_STEP_CYCLES_SHIFT 6
#define NEURAL_CONTROL_MAC_STEP_CYCLES_MASK (0xFC0ul)
#define NEURAL_CONTROL_MAC_STEP_CYCLES_MAX_VALUE 63

/* CSF_CONFIG register */
#define CSF_CONFIG_FORCE_COHERENCY_FEATURES_SHIFT 2

#define MCU_CONTROL_REQ_DISABLE 0x0
#define MCU_CONTROL_REQ_ENABLE 0x1
#define MCU_CONTROL_REQ_AUTO 0x2

#define MCU_STATUS_VALUE_DISABLED 0x0
#define MCU_STATUS_VALUE_ENABLED 0x1
#define MCU_STATUS_VALUE_HALT 0x2
#define MCU_STATUS_VALUE_FATAL 0x3

#define MCU_CNTRL_DOORBELL_DISABLE_SHIFT (31)
#define MCU_CNTRL_DOORBELL_DISABLE_MASK (1 << MCU_CNTRL_DOORBELL_DISABLE_SHIFT)

/* JOB IRQ flags */
#define JOB_IRQ_GLOBAL_IF (1u << 31) /* Global interface interrupt received */

/* GPU_COMMAND codes */
#define GPU_COMMAND_CODE_NOP 0x00 /* No operation, nothing happens */
#define GPU_COMMAND_CODE_RESET 0x01 /* Reset the GPU */
#define GPU_COMMAND_CODE_TIME 0x03 /* Configure time sources */
#define GPU_COMMAND_CODE_FLUSH_CACHES 0x04 /* Flush caches */
#define GPU_COMMAND_CODE_SET_PROTECTED_MODE 0x05 /* Places the GPU in protected mode */
#define GPU_COMMAND_CODE_FINISH_HALT 0x06 /* Halt CSF */
#define GPU_COMMAND_CODE_CLEAR_FAULT 0x07 /* Clear GPU_FAULTSTATUS and GPU_FAULTADDRESS, TODX */
#define GPU_COMMAND_CODE_FLUSH_PA_RANGE 0x08 /* Flush the GPU caches for a physical range, TITX */

/* GPU_COMMAND_RESET payloads */

/* This will leave the state of active jobs UNDEFINED, but will leave the external bus in a defined and idle state.
 * Power domains will remain powered on.
 */
#define GPU_COMMAND_RESET_PAYLOAD_FAST_RESET 0x00

/* This will leave the state of active CSs UNDEFINED, but will leave the external bus in a defined and
 * idle state.
 */
#define GPU_COMMAND_RESET_PAYLOAD_SOFT_RESET 0x01

/* This reset will leave the state of currently active streams UNDEFINED, will likely lose data, and may leave
 * the system bus in an inconsistent state. Use only as a last resort when nothing else works.
 */
#define GPU_COMMAND_RESET_PAYLOAD_HARD_RESET 0x02

/* GPU_COMMAND_TIME payloads */
#define GPU_COMMAND_TIME_DISABLE 0x00 /* Disable cycle counter */
#define GPU_COMMAND_TIME_ENABLE 0x01 /* Enable cycle counter */

/* GPU_COMMAND_FLUSH_CACHES payloads bits for L2 caches */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_NONE 0x000 /* No flush */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_CLEAN 0x001 /* CLN only */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_CLEAN_INVALIDATE 0x003 /* CLN + INV */

/* GPU_COMMAND_FLUSH_CACHES payloads bits for Load-store caches */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_NONE 0x000 /* No flush */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_CLEAN 0x010 /* CLN only */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_CLEAN_INVALIDATE 0x030 /* CLN + INV */

/* GPU_COMMAND_FLUSH_CACHES payloads bits for Other caches */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_NONE 0x000 /* No flush */
#define GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_INVALIDATE 0x200 /* INV only */

/* GPU_COMMAND_FLUSH_PA_RANGE payload bits for flush modes */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_NONE 0x00 /* No flush */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_CLEAN 0x01 /* CLN only */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_INVALIDATE 0x02 /* INV only */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_CLEAN_INVALIDATE 0x03 /* CLN + INV */

/* GPU_COMMAND_FLUSH_PA_RANGE payload bits for which caches should be the target of the command */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_OTHER_CACHE 0x10 /* Other caches */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_LSC_CACHE 0x20 /* Load-store caches */
#define GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_L2_CACHE 0x40 /* L2 caches */

/* GPU_COMMAND command + payload */
#define GPU_COMMAND_CODE_PAYLOAD(opcode, payload) ((__u32)opcode | ((__u32)payload << 8))

/* Final GPU_COMMAND form */
/* No operation, nothing happens */
#define GPU_COMMAND_NOP GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_NOP, 0)

/* Stop all external bus interfaces, and then reset the entire GPU. */
#define GPU_COMMAND_SOFT_RESET \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_RESET, GPU_COMMAND_RESET_PAYLOAD_SOFT_RESET)

/* Immediately reset the entire GPU. */
#define GPU_COMMAND_HARD_RESET \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_RESET, GPU_COMMAND_RESET_PAYLOAD_HARD_RESET)

/* Starts the cycle counter, and system timestamp propagation */
#define GPU_COMMAND_CYCLE_COUNT_START \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_TIME, GPU_COMMAND_TIME_ENABLE)

/* Stops the cycle counter, and system timestamp propagation */
#define GPU_COMMAND_CYCLE_COUNT_STOP \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_TIME, GPU_COMMAND_TIME_DISABLE)

/* Clean and invalidate L2 cache (Equivalent to FLUSH_PT) */
#define GPU_COMMAND_CACHE_CLN_INV_L2                                                     \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES,                          \
				 (GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_NONE |            \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_NONE))

/* Clean and invalidate L2 and LSC caches (Equivalent to FLUSH_MEM) */
#define GPU_COMMAND_CACHE_CLN_INV_L2_LSC                                                  \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES,                           \
				 (GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_CLEAN_INVALIDATE |  \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_NONE))

/* Clean and invalidate L2, LSC, and Other caches */
#define GPU_COMMAND_CACHE_CLN_INV_FULL                                                    \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES,                           \
				 (GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_CLEAN_INVALIDATE |  \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_INVALIDATE))

/* Clean and invalidate only LSC cache */
#define GPU_COMMAND_CACHE_CLN_INV_LSC                                                     \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES,                           \
				 (GPU_COMMAND_FLUSH_CACHES_PAYLOAD_L2_NONE |              \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_LSC_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_CACHES_PAYLOAD_OTHER_NONE))

/* Clean and invalidate physical range L2 cache (equivalent to FLUSH_PT) */
#define GPU_COMMAND_FLUSH_PA_RANGE_CLN_INV_L2                                                \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_PA_RANGE,                            \
				 (GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_L2_CACHE))

/* Clean and invalidate physical range L2 and LSC cache (equivalent to FLUSH_MEM) */
#define GPU_COMMAND_FLUSH_PA_RANGE_CLN_INV_L2_LSC                                            \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_PA_RANGE,                            \
				 (GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_LSC_CACHE |             \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_L2_CACHE))

/* Clean and invalidate physical range L2, LSC and Other caches */
#define GPU_COMMAND_FLUSH_PA_RANGE_CLN_INV_FULL                                              \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_PA_RANGE,                            \
				 (GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_MODE_CLEAN_INVALIDATE | \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_OTHER_CACHE |           \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_LSC_CACHE |             \
				  GPU_COMMAND_FLUSH_PA_RANGE_PAYLOAD_L2_CACHE))

/* Merge cache flush commands */
#define GPU_COMMAND_FLUSH_CACHE_MERGE(cmd1, cmd2) ((cmd1) | (cmd2))

/* Places the GPU in protected mode */
#define GPU_COMMAND_SET_PROTECTED_MODE \
	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_SET_PROTECTED_MODE, 0)

/* Halt CSF */
#define GPU_COMMAND_FINISH_HALT GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FINISH_HALT, 0)

/* Clear GPU faults */
#define GPU_COMMAND_CLEAR_FAULT GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_CLEAR_FAULT, 0)

/* End Command Values */

/* GPU_FAULTSTATUS register */
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT 0
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK (0xFFul)
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GET(reg_val) \
	(((reg_val)&GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) >> GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT)
#define GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT 8
#define GPU_FAULTSTATUS_ACCESS_TYPE_MASK (0x3ul << GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT)

#define GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT GPU_U(10)
#define GPU_FAULTSTATUS_ADDRESS_VALID_MASK (GPU_U(0x1) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT)

#define GPU_FAULTSTATUS_JASID_VALID_SHIFT GPU_U(11)
#define GPU_FAULTSTATUS_JASID_VALID_MASK (GPU_U(0x1) << GPU_FAULTSTATUS_JASID_VALID_SHIFT)

#define GPU_FAULTSTATUS_JASID_SHIFT 12
#define GPU_FAULTSTATUS_JASID_MASK (0xF << GPU_FAULTSTATUS_JASID_SHIFT)
#define GPU_FAULTSTATUS_JASID_GET(reg_val) \
	(((reg_val)&GPU_FAULTSTATUS_JASID_MASK) >> GPU_FAULTSTATUS_JASID_SHIFT)
#define GPU_FAULTSTATUS_JASID_SET(reg_val, value)    \
	(((reg_val) & ~GPU_FAULTSTATUS_JASID_MASK) | \
	 (((value) << GPU_FAULTSTATUS_JASID_SHIFT) & GPU_FAULTSTATUS_JASID_MASK))

#define GPU_FAULTSTATUS_SOURCE_ID_SHIFT 16
#define GPU_FAULTSTATUS_SOURCE_ID_MASK (0xFFFFul << GPU_FAULTSTATUS_SOURCE_ID_SHIFT)
/* End GPU_FAULTSTATUS register */

/* GPU_FAULTSTATUS_ACCESS_TYPE values */
#define GPU_FAULTSTATUS_ACCESS_TYPE_ATOMIC 0x0
#define GPU_FAULTSTATUS_ACCESS_TYPE_EXECUTE 0x1
#define GPU_FAULTSTATUS_ACCESS_TYPE_READ 0x2
#define GPU_FAULTSTATUS_ACCESS_TYPE_WRITE 0x3
/* End of GPU_FAULTSTATUS_ACCESS_TYPE values */

/* Implementation-dependent exception codes used to indicate CSG
 * and CS errors that are not specified in the specs.
 */
#define GPU_EXCEPTION_TYPE_SW_FAULT_0 ((__u8)0x70)
#define GPU_EXCEPTION_TYPE_SW_FAULT_1 ((__u8)0x71)
#define GPU_EXCEPTION_TYPE_SW_FAULT_2 ((__u8)0x72)

/* GPU_FAULTSTATUS_EXCEPTION_TYPE values */
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_OK 0x00
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_BUS_FAULT 0x80
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_SHAREABILITY_FAULT 0x88
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_SYSTEM_SHAREABILITY_FAULT 0x89
#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_CACHEABILITY_FAULT 0x8A
/* End of GPU_FAULTSTATUS_EXCEPTION_TYPE values */

#define GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT GPU_U(10)
#define GPU_FAULTSTATUS_ADDRESS_VALID_MASK (GPU_U(0x1) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT)
#define GPU_FAULTSTATUS_ADDRESS_VALID_GET(reg_val) \
	(((reg_val)&GPU_FAULTSTATUS_ADDRESS_VALID_MASK) >> GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT)
#define GPU_FAULTSTATUS_ADDRESS_VALID_SET(reg_val, value)    \
	(((reg_val) & ~GPU_FAULTSTATUS_ADDRESS_VALID_MASK) | \
	 (((value) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT) & GPU_FAULTSTATUS_ADDRESS_VALID_MASK))

/* GPU IRQ flags */
#define GPU_FAULT (1U << 0) /* A GPU Fault has occurred */
#define GPU_PROTECTED_FAULT (1U << 1) /* A GPU fault has occurred in protected mode */


#define RESET_COMPLETED (1U << 8) /* Set when a reset has completed.  */
#define POWER_CHANGED_SINGLE \
	(1U << 9) /* Set when a single core has finished powering up or down. */
#define POWER_CHANGED_ALL (1U << 10) /* Set when all cores have finished powering up or down. */
#define CLEAN_CACHES_COMPLETED (1U << 17) /* Set when a cache clean operation has completed. */
#define DOORBELL_MIRROR (1U << 18) /* Mirrors the doorbell interrupt line to the CPU */
#define MCU_STATUS_GPU_IRQ (1U << 19) /* MCU requires attention */
#define FLUSH_PA_RANGE_COMPLETED \
	(1U << 20) /* Set when a physical range cache clean operation has completed. */

#define PWR_IRQ_POWER_CHANGED_SINGLE_SHIFT 0
#define PWR_IRQ_POWER_CHANGED_ALL_SHIFT 1
#define PWR_IRQ_DELEGATION_CHANGED_SHIFT 2
#define PWR_IRQ_RESET_COMPLETED_SHIFT 3
#define PWR_IRQ_RETRACT_COMPLETED_SHIFT 4
#define PWR_IRQ_INSPECT_COMPLETED_SHIFT 5
/* PWR_IRQ flags */
#define PWR_IRQ_POWER_CHANGED_SINGLE (1 << PWR_IRQ_POWER_CHANGED_SINGLE_SHIFT)
#define PWR_IRQ_POWER_CHANGED_ALL (1 << PWR_IRQ_POWER_CHANGED_ALL_SHIFT)
#define PWR_IRQ_DELEGATION_CHANGED (1 << PWR_IRQ_DELEGATION_CHANGED_SHIFT)
#define PWR_IRQ_RESET_COMPLETED (1 << PWR_IRQ_RESET_COMPLETED_SHIFT)
#define PWR_IRQ_RETRACT_COMPLETED (1 << PWR_IRQ_RETRACT_COMPLETED_SHIFT)
#define PWR_IRQ_INSPECT_COMPLETED (1 << PWR_IRQ_INSPECT_COMPLETED_SHIFT)
#define PWR_IRQ_COMMAND_NOT_ALLOWED (1 << 6)
#define PWR_IRQ_COMMAND_INVALID (1 << 7)

#define PWR_IRQ_REG_COMMON                                                                     \
	(PWR_IRQ_POWER_CHANGED_ALL | PWR_IRQ_DELEGATION_CHANGED | PWR_IRQ_RESET_COMPLETED |    \
	 PWR_IRQ_RETRACT_COMPLETED | PWR_IRQ_INSPECT_COMPLETED | PWR_IRQ_COMMAND_NOT_ALLOWED | \
	 PWR_IRQ_COMMAND_INVALID)

#define PWR_IRQ_REG_SINGLE (PWR_IRQ_POWER_CHANGED_SINGLE | PWR_IRQ_REG_COMMON)

#define PWR_IRQ_REG_ALL (IS_ENABLED(CONFIG_MALI_DEBUG) ? PWR_IRQ_REG_SINGLE : PWR_IRQ_REG_COMMON)

/* GPU_FEATURES register */
#define GPU_FEATURES_RAY_TRACING_SHIFT GPU_U(2)
#define GPU_FEATURES_RAY_TRACING_MASK (GPU_U(0x1) << GPU_FEATURES_RAY_TRACING_SHIFT)
#define GPU_FEATURES_RAY_TRACING_GET(reg_val) \
	(((reg_val)&GPU_FEATURES_RAY_TRACING_MASK) >> GPU_FEATURES_RAY_TRACING_SHIFT)
/* End of GPU_FEATURES register */

#define GPU_IRQ_REG_COMMON (GPU_FAULT | GPU_PROTECTED_FAULT | MCU_STATUS_GPU_IRQ)

/* GPU_FEATURES register cont*/
#define GPU_FEATURES_NEURAL_ENGINE_SHIFT GPU_U(4)
#define GPU_FEATURES_NEURAL_ENGINE_MASK (GPU_U(0x1) << GPU_FEATURES_NEURAL_ENGINE_SHIFT)
/* End of GPU_FEATURES register */

/* PWR_STATUS register */
#define PWR_STATUS_ALLOW_L2_SHIFT GPU_U(0)
#define PWR_STATUS_ALLOW_L2_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_L2_SHIFT)
#define PWR_STATUS_ALLOW_L2_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_L2_MASK) >> PWR_STATUS_ALLOW_L2_SHIFT)
#define PWR_STATUS_ALLOW_L2_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_L2_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_L2_SHIFT) & PWR_STATUS_ALLOW_L2_MASK))
#define PWR_STATUS_ALLOW_TILER_SHIFT GPU_U(1)
#define PWR_STATUS_ALLOW_TILER_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_TILER_SHIFT)
#define PWR_STATUS_ALLOW_TILER_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_TILER_MASK) >> PWR_STATUS_ALLOW_TILER_SHIFT)
#define PWR_STATUS_ALLOW_TILER_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_TILER_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_TILER_SHIFT) & PWR_STATUS_ALLOW_TILER_MASK))
#define PWR_STATUS_ALLOW_SHADER_SHIFT GPU_U(8)
#define PWR_STATUS_ALLOW_SHADER_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_SHADER_SHIFT)
#define PWR_STATUS_ALLOW_SHADER_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_SHADER_MASK) >> PWR_STATUS_ALLOW_SHADER_SHIFT)
#define PWR_STATUS_ALLOW_SHADER_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_SHADER_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_SHADER_SHIFT) & PWR_STATUS_ALLOW_SHADER_MASK))
#define PWR_STATUS_ALLOW_NEURAL_SHIFT GPU_U(9)
#define PWR_STATUS_ALLOW_NEURAL_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_NEURAL_SHIFT)
#define PWR_STATUS_ALLOW_NEURAL_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_NEURAL_MASK) >> PWR_STATUS_ALLOW_NEURAL_SHIFT)
#define PWR_STATUS_ALLOW_NEURAL_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_NEURAL_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_NEURAL_SHIFT) & PWR_STATUS_ALLOW_NEURAL_MASK))
#define PWR_STATUS_ALLOW_BASE_SHIFT GPU_U(14)
#define PWR_STATUS_ALLOW_BASE_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_BASE_SHIFT)
#define PWR_STATUS_ALLOW_BASE_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_BASE_MASK) >> PWR_STATUS_ALLOW_BASE_SHIFT)
#define PWR_STATUS_ALLOW_BASE_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_BASE_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_BASE_SHIFT) & PWR_STATUS_ALLOW_BASE_MASK))
#define PWR_STATUS_ALLOW_STACK_SHIFT GPU_U(15)
#define PWR_STATUS_ALLOW_STACK_MASK (GPU_U(0x1) << PWR_STATUS_ALLOW_STACK_SHIFT)
#define PWR_STATUS_ALLOW_STACK_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_STACK_MASK) >> PWR_STATUS_ALLOW_STACK_SHIFT)
#define PWR_STATUS_ALLOW_STACK_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_ALLOW_STACK_MASK) | \
	 (((value) << PWR_STATUS_ALLOW_STACK_SHIFT) & PWR_STATUS_ALLOW_STACK_MASK))
#define PWR_STATUS_DELEGATED_L2_SHIFT GPU_U(16)
#define PWR_STATUS_DELEGATED_L2_MASK (GPU_U(0x1) << PWR_STATUS_DELEGATED_L2_SHIFT)
#define PWR_STATUS_DELEGATED_L2_GET(reg_val) \
	(((reg_val)&PWR_STATUS_DELEGATED_L2_MASK) >> PWR_STATUS_DELEGATED_L2_SHIFT)
#define PWR_STATUS_DELEGATED_L2_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_DELEGATED_L2_MASK) | \
	 (((value) << PWR_STATUS_DELEGATED_L2_SHIFT) & PWR_STATUS_DELEGATED_L2_MASK))
#define PWR_STATUS_DELEGATED_TILER_SHIFT GPU_U(17)
#define PWR_STATUS_DELEGATED_TILER_MASK (GPU_U(0x1) << PWR_STATUS_DELEGATED_TILER_SHIFT)
#define PWR_STATUS_DELEGATED_TILER_GET(reg_val) \
	(((reg_val)&PWR_STATUS_DELEGATED_TILER_MASK) >> PWR_STATUS_DELEGATED_TILER_SHIFT)
#define PWR_STATUS_DELEGATED_TILER_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_DELEGATED_TILER_MASK) | \
	 (((value) << PWR_STATUS_DELEGATED_TILER_SHIFT) & PWR_STATUS_DELEGATED_TILER_MASK))
#define PWR_STATUS_DELEGATED_SHADER_SHIFT GPU_U(24)
#define PWR_STATUS_DELEGATED_SHADER_MASK (GPU_U(0x1) << PWR_STATUS_DELEGATED_SHADER_SHIFT)
#define PWR_STATUS_DELEGATED_SHADER_GET(reg_val) \
	(((reg_val)&PWR_STATUS_DELEGATED_SHADER_MASK) >> PWR_STATUS_DELEGATED_SHADER_SHIFT)
#define PWR_STATUS_DELEGATED_SHADER_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_DELEGATED_SHADER_MASK) | \
	 (((value) << PWR_STATUS_DELEGATED_SHADER_SHIFT) & PWR_STATUS_DELEGATED_SHADER_MASK))
#define PWR_STATUS_DELEGATED_NEURAL_SHIFT GPU_U(25)
#define PWR_STATUS_DELEGATED_NEURAL_MASK (GPU_U(0x1) << PWR_STATUS_DELEGATED_NEURAL_SHIFT)
#define PWR_STATUS_DELEGATED_NEURAL_GET(reg_val) \
	(((reg_val)&PWR_STATUS_DELEGATED_NEURAL_MASK) >> PWR_STATUS_DELEGATED_NEURAL_SHIFT)
#define PWR_STATUS_DELEGATED_NEURAL_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_STATUS_DELEGATED_NEURAL_MASK) | \
	 (((value) << PWR_STATUS_DELEGATED_NEURAL_SHIFT) & PWR_STATUS_DELEGATED_NEURAL_MASK))
#define PWR_IRQ_COMMAND_NOT_ALLOWED_SHIFT GPU_U(30)
#define PWR_IRQ_COMMAND_NOT_ALLOWED_MASK (GPU_U(0x1) << PWR_IRQ_COMMAND_NOT_ALLOWED_SHIFT)
#define PWR_IRQ_COMMAND_NOT_ALLOWED_GET(reg_val) \
	(((reg_val)&PWR_IRQ_COMMAND_NOT_ALLOWED_MASK) >> PWR_IRQ_COMMAND_NOT_ALLOWED_SHIFT)
#define PWR_IRQ_COMMAND_NOT_ALLOWED_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_IRQ_COMMAND_NOT_ALLOWED_MASK) | \
	 (((value) << PWR_IRQ_COMMAND_NOT_ALLOWED_SHIFT) & PWR_IRQ_COMMAND_NOT_ALLOWED_MASK))
#define PWR_IRQ_COMMAND_INVALID_SHIFT GPU_U(31)
#define PWR_IRQ_COMMAND_INVALID_MASK (GPU_U(0x1) << PWR_IRQ_COMMAND_INVALID_SHIFT)
#define PWR_IRQ_COMMAND_INVALID_GET(reg_val) \
	(((reg_val)&PWR_IRQ_COMMAND_INVALID_MASK) >> PWR_IRQ_COMMAND_INVALID_SHIFT)
#define PWR_IRQ_COMMAND_INVALID_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_IRQ_COMMAND_INVALID_MASK) | \
	 (((value) << PWR_IRQ_COMMAND_INVALID_SHIFT) & PWR_IRQ_COMMAND_INVALID_MASK))
#define PWR_STATUS_ALLOW_HARD_RESET_SHIFT GPU_U(32)
#define PWR_STATUS_ALLOW_HARD_RESET_MASK (GPU_ULL(0x1) << PWR_STATUS_ALLOW_HARD_RESET_SHIFT)
#define PWR_STATUS_ALLOW_HARD_RESET_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_HARD_RESET_MASK) >> PWR_STATUS_ALLOW_HARD_RESET_SHIFT)
#define PWR_STATUS_ALLOW_HARD_RESET_SET(reg_val, value)              \
	(~(~(reg_val) | PWR_STATUS_ALLOW_HARD_RESET_MASK) |          \
	 (((uint64_t)(value) << PWR_STATUS_ALLOW_HARD_RESET_SHIFT) & \
	  PWR_STATUS_ALLOW_HARD_RESET_MASK))
#define PWR_STATUS_ALLOW_SOFT_RESET_SHIFT GPU_U(33)
#define PWR_STATUS_ALLOW_SOFT_RESET_MASK (GPU_ULL(0x1) << PWR_STATUS_ALLOW_SOFT_RESET_SHIFT)
#define PWR_STATUS_ALLOW_SOFT_RESET_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_SOFT_RESET_MASK) >> PWR_STATUS_ALLOW_SOFT_RESET_SHIFT)
#define PWR_STATUS_ALLOW_SOFT_RESET_SET(reg_val, value)              \
	(~(~(reg_val) | PWR_STATUS_ALLOW_SOFT_RESET_MASK) |          \
	 (((uint64_t)(value) << PWR_STATUS_ALLOW_SOFT_RESET_SHIFT) & \
	  PWR_STATUS_ALLOW_SOFT_RESET_MASK))
#define PWR_STATUS_ALLOW_FAST_RESET_SHIFT GPU_U(34)
#define PWR_STATUS_ALLOW_FAST_RESET_MASK (GPU_ULL(0x1) << PWR_STATUS_ALLOW_FAST_RESET_SHIFT)
#define PWR_STATUS_ALLOW_FAST_RESET_GET(reg_val) \
	(((reg_val)&PWR_STATUS_ALLOW_FAST_RESET_MASK) >> PWR_STATUS_ALLOW_FAST_RESET_SHIFT)
#define PWR_STATUS_POWER_PENDING_SHIFT GPU_U(41)
#define PWR_STATUS_POWER_PENDING_MASK (GPU_ULL(0x1) << PWR_STATUS_POWER_PENDING_SHIFT)
#define PWR_STATUS_POWER_PENDING_GET(reg_val) \
	(((reg_val)&PWR_STATUS_POWER_PENDING_MASK) >> PWR_STATUS_POWER_PENDING_SHIFT)
#define PWR_STATUS_RESET_PENDING_SHIFT GPU_U(42)
#define PWR_STATUS_RESET_PENDING_MASK (GPU_ULL(0x1) << PWR_STATUS_RESET_PENDING_SHIFT)
#define PWR_STATUS_RESET_PENDING_GET(reg_val) \
	(((reg_val)&PWR_STATUS_RESET_PENDING_MASK) >> PWR_STATUS_RESET_PENDING_SHIFT)
#define PWR_STATUS_RETRACT_PENDING_SHIFT GPU_U(43)
#define PWR_STATUS_RETRACT_PENDING_MASK (GPU_ULL(0x1) << PWR_STATUS_RETRACT_PENDING_SHIFT)
#define PWR_STATUS_RETRACT_PENDING_GET(reg_val) \
	(((reg_val)&PWR_STATUS_RETRACT_PENDING_MASK) >> PWR_STATUS_RETRACT_PENDING_SHIFT)
#define PWR_STATUS_INSPECT_PENDING_SHIFT GPU_U(44)
#define PWR_STATUS_INSPECT_PENDING_MASK (GPU_ULL(0x1) << PWR_STATUS_INSPECT_PENDING_SHIFT)
#define PWR_STATUS_INSPECT_PENDING_GET(reg_val) \
	(((reg_val)&PWR_STATUS_INSPECT_PENDING_MASK) >> PWR_STATUS_INSPECT_PENDING_SHIFT)

/* PWR_COMMAND register */
#define PWR_COMMAND_COMMAND_SHIFT GPU_U(0)
#define PWR_COMMAND_COMMAND_MASK (GPU_U(0xFF) << PWR_COMMAND_COMMAND_SHIFT)
#define PWR_COMMAND_COMMAND_GET(reg_val) \
	(((reg_val)&PWR_COMMAND_COMMAND_MASK) >> PWR_COMMAND_COMMAND_SHIFT)
#define PWR_COMMAND_COMMAND_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_COMMAND_COMMAND_MASK) | \
	 (((value) << PWR_COMMAND_COMMAND_SHIFT) & PWR_COMMAND_COMMAND_MASK))
/* PWR_COMMAND_COMMAND values */
#define PWR_COMMAND_COMMAND_NOP 0x0
#define PWR_COMMAND_COMMAND_POWER_UP 0x10
#define PWR_COMMAND_COMMAND_POWER_DOWN 0x11
#define PWR_COMMAND_COMMAND_DELEGATE 0x20
#define PWR_COMMAND_COMMAND_RETRACT 0x21
#define PWR_COMMAND_COMMAND_RESET_HARD 0x30
#define PWR_COMMAND_COMMAND_RESET_SOFT 0x31
#define PWR_COMMAND_COMMAND_RESET_FAST 0x32
#define PWR_COMMAND_COMMAND_INSPECT 0xF0
/* End of PWR_COMMAND_COMMAND values */
#define PWR_COMMAND_DOMAIN_SHIFT GPU_U(8)
#define PWR_COMMAND_DOMAIN_MASK (GPU_U(0xF) << PWR_COMMAND_DOMAIN_SHIFT)
#define PWR_COMMAND_DOMAIN_GET(reg_val) \
	(((reg_val)&PWR_COMMAND_DOMAIN_MASK) >> PWR_COMMAND_DOMAIN_SHIFT)
#define PWR_COMMAND_DOMAIN_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_COMMAND_DOMAIN_MASK) | \
	 (((value) << PWR_COMMAND_DOMAIN_SHIFT) & PWR_COMMAND_DOMAIN_MASK))
/* PWR_COMMAND_DOMAIN values */
#define PWR_COMMAND_DOMAIN_L2 0x0
#define PWR_COMMAND_DOMAIN_TILER 0x1
#define PWR_COMMAND_DOMAIN_SHADER 0x8
#define PWR_COMMAND_DOMAIN_NEURAL 0x9
#define PWR_COMMAND_DOMAIN_BASE 0xE
#define PWR_COMMAND_DOMAIN_STACK 0xF
/* End of PWR_COMMAND_DOMAIN values */
#define PWR_COMMAND_SUBDOMAIN_SHIFT GPU_U(16)
#define PWR_COMMAND_SUBDOMAIN_MASK (GPU_U(0xFF) << PWR_COMMAND_SUBDOMAIN_SHIFT)
#define PWR_COMMAND_SUBDOMAIN_GET(reg_val) \
	(((reg_val)&PWR_COMMAND_SUBDOMAIN_MASK) >> PWR_COMMAND_SUBDOMAIN_SHIFT)
#define PWR_COMMAND_SUBDOMAIN_SET(reg_val, value)     \
	(~(~(reg_val) | PWR_COMMAND_SUBDOMAIN_MASK) | \
	 (((value) << PWR_COMMAND_SUBDOMAIN_SHIFT) & PWR_COMMAND_SUBDOMAIN_MASK))

#define VALIDATE_POWER_REG_ENUM(power_reg) \
	BUILD_BUG_ON((u32)HOST_POWER_ENUM(power_reg) != (u32)GPU_CONTROL_ENUM(power_reg))

#define VALIDATE_POWER_REG_ENUMS(core)                     \
	do {                                               \
		VALIDATE_POWER_REG_ENUM(core##_READY);     \
		VALIDATE_POWER_REG_ENUM(core##_PRESENT);   \
		VALIDATE_POWER_REG_ENUM(core##_PWRTRANS);  \
		VALIDATE_POWER_REG_ENUM(core##_PWRACTIVE); \
	} while (0)

#define VALIDATE_CORESTACK_POWER_REG_ENUMS               \
	do {                                             \
		VALIDATE_POWER_REG_ENUM(STACK_READY);    \
		VALIDATE_POWER_REG_ENUM(STACK_PRESENT);  \
		VALIDATE_POWER_REG_ENUM(STACK_PWRTRANS); \
	} while (0)

#define VALIDATE_ALL_POWER_REG_ENUMS                \
	do {                                        \
		VALIDATE_POWER_REG_ENUMS(L2);       \
		VALIDATE_POWER_REG_ENUMS(TILER);    \
		VALIDATE_POWER_REG_ENUMS(SHADER);   \
		VALIDATE_CORESTACK_POWER_REG_ENUMS; \
	} while (0)


#endif /* _MALI_KBASE_REGMAP_CSF_MACROS_H_ */
